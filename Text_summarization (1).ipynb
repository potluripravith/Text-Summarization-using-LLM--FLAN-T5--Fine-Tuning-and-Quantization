{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries and Load Pre-Trained Model"
      ],
      "metadata": {
        "id": "UuTTbDEo8Uqt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft bitsandbytes accelerate sentencepiece rouge-score chromadb langchain sentence-transformers prettytable torch numpy pandas tqdm"
      ],
      "metadata": {
        "collapsed": true,
        "id": "S2lwlEmDdwyF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community"
      ],
      "metadata": {
        "id": "CxhMB-_tWSm9",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access stored files\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "DAu-ffFR0NOk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, BitsAndBytesConfig, Seq2SeqTrainingArguments, Seq2SeqTrainer, pipeline\n",
        "from peft import get_peft_model, LoraConfig, TaskType, prepare_model_for_kbit_training\n",
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.document_loaders import TextLoader\n",
        "from rouge_score import rouge_scorer\n",
        "from prettytable import PrettyTable\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import bitsandbytes as bnb\n",
        "import chromadb\n",
        "import torch\n",
        "import pandas as pd\n",
        "import time\n",
        "import sys\n"
      ],
      "metadata": {
        "id": "dtruwZEi-v9E"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "print(torch.cuda.is_available())"
      ],
      "metadata": {
        "id": "Iw4h5sB2DjtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Dataset"
      ],
      "metadata": {
        "id": "ZszK47Qo8iz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the CNN/DailyMail dataset\n",
        "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
        "# Shuffle the dataset with a fixed seed for reproducibility\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "# Select a subset of the dataset for training, validation, and testing\n",
        "train_dataset = dataset[\"train\"].select(range(8000))\n",
        "val_dataset = dataset[\"validation\"].select(range(1000))\n",
        "test_dataset = dataset[\"test\"].select(range(1000))"
      ],
      "metadata": {
        "id": "z9KZdXD38wvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Preprocessing"
      ],
      "metadata": {
        "id": "DgVwQZu28io3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check dataset shape\n",
        "train_dataset.shape"
      ],
      "metadata": {
        "id": "5afB7lAcAQEI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ],
      "metadata": {
        "id": "b8ElmAa5Ae_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to preprocess text data\n",
        "def preprocess_function(examples):\n",
        "    # Format inputs as \"Summarize: {article}\"\n",
        "    inputs = [\"Summarize: \" + doc for doc in examples[\"article\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    # Tokenize the summaries\n",
        "    labels = tokenizer(\n",
        "        examples[\"highlights\"],\n",
        "        max_length=128,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\"\n",
        "    )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to training and validation datasets\n",
        "processed_train_dataset = train_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=train_dataset.column_names\n",
        ")\n",
        "processed_val_dataset = val_dataset.map(\n",
        "    preprocess_function,\n",
        "    batched=True,\n",
        "    remove_columns=val_dataset.column_names\n",
        ")"
      ],
      "metadata": {
        "id": "FH72cmLz9N9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Base Model with Quantization"
      ],
      "metadata": {
        "id": "kr2e4RHh80_Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up quantization to load model efficiently\n",
        "quantization_config = BitsAndBytesConfig(load_in_8bit=True)"
      ],
      "metadata": {
        "id": "BinxiaayDCjO"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained FLAN-T5 base model with quantization\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    \"google/flan-t5-base\",\n",
        "    quantization_config=quantization_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare model for parameter-efficient fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Configure LoRA for fine-tuning\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=16,                          # Rank of update matrices\n",
        "    lora_alpha=32,                 # Scale parameter\n",
        "    lora_dropout=0.1,              # Dropout probability\n",
        "    target_modules=[\"q\", \"v\"],     # Apply LoRA to attention query and value matrices\n",
        ")\n",
        "\n",
        "# Apply LoRA adapters\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()  # Should show ~1-3% of parameters are trainable\n"
      ],
      "metadata": {
        "id": "ewI6II7gBMcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-Tuning Setup"
      ],
      "metadata": {
        "id": "J5Q02xme8iwh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./flan-t5-summarization\", # Output directory for model checkpoints\n",
        "    evaluation_strategy=\"steps\",          # Evaluate at regular intervals\n",
        "    eval_steps=500,                       # Number of steps between evaluations\n",
        "    learning_rate=5e-4,                   # Learning rate for optimization\n",
        "    per_device_train_batch_size=4,        # Training batch size per device\n",
        "    per_device_eval_batch_size=4,         # Evaluation batch size per device\n",
        "    weight_decay=0.01,                    # Weight decay to prevent overfitting\n",
        "    save_total_limit=3,                   # Keep only the last 3 checkpoints\n",
        "    num_train_epochs=3,                   # Number of training epochs\n",
        "    predict_with_generate=True,           # Use text generation during evaluation\n",
        "    fp16=True,                            # Enable mixed precision training\n",
        "    gradient_accumulation_steps=4,        # Effective batch size through accumulation\n",
        "    generation_max_length=128,            # Maximum generated summary length\n",
        "    report_to=\"tensorboard\",              #Log metrics to TensorBoard\n",
        ")\n",
        "\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=processed_train_dataset,\n",
        "    eval_dataset=processed_val_dataset,\n",
        "    tokenizer=tokenizer,\n",
        ")\n",
        "\n",
        "\n",
        "# start and save the Fine-tune the model\n",
        "trainer.train()\n",
        "model.save_pretrained(\"/content/drive/MyDrive/flan-t5-summarization-final\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/flan-t5-summarization-final\")"
      ],
      "metadata": {
        "id": "CZ4NWGvx85m3",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate Model Performance"
      ],
      "metadata": {
        "id": "Pa1FZLR28icY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create embedding model (using all-MiniLM-L6 for efficiency)\n",
        "embedding_model = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "# Setup vector database with ChromaDB for retrieval-augmented generation (RAG)\n",
        "chroma_client = chromadb.PersistentClient(\"./chroma_db\")\n",
        "vector_db = Chroma(\n",
        "    client=chroma_client,\n",
        "    collection_name=\"summarization_knowledge_base\",\n",
        "    embedding_function=embedding_model\n",
        ")\n",
        "\n",
        "# Function to load and process knowledge base documents\n",
        "# This splits large documents into manageable chunks before adding them to the vector DB\n",
        "def process_knowledge_documents(documents_path):\n",
        "    text_splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=1000,\n",
        "        chunk_overlap=200\n",
        "    )\n",
        "    loader = TextLoader(documents_path)\n",
        "    documents = loader.load()\n",
        "    splits = text_splitter.split_documents(documents)\n",
        "    vector_db.add_documents(splits)\n",
        "\n",
        "# Function to generate summaries using RAG-enhanced approach\n",
        "def rag_enhanced_summarization(article_text, fine_tuned_model, top_k=3):\n",
        "    # Retrieve relevant context from knowledge base\n",
        "    retrieved_docs = vector_db.similarity_search(article_text, k=top_k)\n",
        "    relevant_context = \"\\n\".join([doc.page_content for doc in retrieved_docs])\n",
        "\n",
        "     # Enhance the input prompt with retrieved context\n",
        "    enhanced_prompt = f\"\"\"Summarize this article using the provided context where relevant.\n",
        "\n",
        "    Context: {relevant_context}\n",
        "\n",
        "    Article: {article_text}\n",
        "    \"\"\"\n",
        "\n",
        "    # 3. Generate summary using fine-tuned model\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=fine_tuned_model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    summary = summarizer(\n",
        "        enhanced_prompt,\n",
        "        max_length=128,\n",
        "        min_length=30,\n",
        "        do_sample=False\n",
        "    )[0][\"summary_text\"]\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "d1KBx0xuWAVG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute ROUGE Scores"
      ],
      "metadata": {
        "id": "Dx1JaY6E90ND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize ROUGE scorer to measure summarization quality\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Function to evaluate summarization models\n",
        "def evaluate_model(model, test_dataset, model_name, use_rag=False):\n",
        "    # Load the model\n",
        "    summarizer = pipeline(\n",
        "        \"summarization\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1\n",
        "    )\n",
        "\n",
        "    results = []\n",
        "    rouge_scores = []\n",
        "    inference_times = []\n",
        "     # Iterate through test dataset and evaluate model performance\n",
        "    for i, example in enumerate(tqdm(test_dataset)):\n",
        "        article = example[\"article\"]\n",
        "        reference_summary = example[\"highlights\"]\n",
        "\n",
        "        # Measure inference time\n",
        "        start_time = time.time()\n",
        "\n",
        "        if use_rag:\n",
        "            generated_summary = rag_enhanced_summarization(article, model)\n",
        "        else:\n",
        "            input_text = \"Summarize: \" + article\n",
        "            generated_summary = summarizer(\n",
        "                input_text,\n",
        "                max_length=128,\n",
        "                min_length=30,\n",
        "                do_sample=False\n",
        "            )[0][\"summary_text\"]\n",
        "\n",
        "        inference_time = time.time() - start_time\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "        # Calculate ROUGE scores\n",
        "        scores = scorer.score(reference_summary, generated_summary)\n",
        "        rouge_scores.append({\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure,\n",
        "            'rougeL': scores['rougeL'].fmeasure\n",
        "        })\n",
        "\n",
        "        # Save evaluation results\n",
        "        results.append({\n",
        "            'article': article[:200] + \"...\",  # Truncate for display\n",
        "            'reference': reference_summary,\n",
        "            'generated': generated_summary,\n",
        "            'rouge1': scores['rouge1'].fmeasure,\n",
        "            'rouge2': scores['rouge2'].fmeasure,\n",
        "            'rougeL': scores['rougeL'].fmeasure,\n",
        "            'inference_time': inference_time\n",
        "        })\n",
        "\n",
        "        # Only process a subset for efficiency\n",
        "        if i >= 99:  # Process 100 examples\n",
        "            break\n",
        "\n",
        "    # Calculate average scores\n",
        "    avg_rouge1 = np.mean([s['rouge1'] for s in rouge_scores])\n",
        "    avg_rouge2 = np.mean([s['rouge2'] for s in rouge_scores])\n",
        "    avg_rougeL = np.mean([s['rougeL'] for s in rouge_scores])\n",
        "    avg_inference_time = np.mean(inference_times)\n",
        "\n",
        "    print(f\"--- Evaluation Results for {model_name} {'with RAG' if use_rag else ''} ---\")\n",
        "    print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
        "    print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
        "    print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
        "    print(f\"Average inference time: {avg_inference_time:.4f} seconds\")\n",
        "\n",
        "\n",
        "    return pd.DataFrame(results), {\n",
        "        'model': model_name,\n",
        "        'rag': use_rag,\n",
        "        'rouge1': avg_rouge1,\n",
        "        'rouge2': avg_rouge2,\n",
        "        'rougeL': avg_rougeL,\n",
        "        'inference_time': avg_inference_time\n",
        "    }\n",
        "\n",
        "# Run comprehensive evaluation\n",
        "eval_results = []\n",
        "\n",
        "\n",
        "# 1. Evaluate base model\n",
        "base_results, base_metrics = evaluate_model(\n",
        "    \"google/flan-t5-base\",\n",
        "    test_dataset,\n",
        "    \"FLAN-T5-Base\"\n",
        ")\n",
        "eval_results.append(base_metrics)\n",
        "\n",
        "# 2. Evaluate fine-tuned model\n",
        "ft_results, ft_metrics = evaluate_model(\n",
        "    \"/content/drive/MyDrive/flan-t5-summarization-final\",\n",
        "    test_dataset,\n",
        "    \"Fine-tuned FLAN-T5\"\n",
        ")\n",
        "eval_results.append(ft_metrics)\n",
        "\n",
        "# 3. Evaluate RAG-enhanced model\n",
        "rag_results, rag_metrics = evaluate_model(\n",
        "    \"/content/drive/MyDrive/flan-t5-summarization-final\",\n",
        "    test_dataset,\n",
        "    \"Fine-tuned FLAN-T5\",\n",
        "    use_rag=True\n",
        ")\n",
        "eval_results.append(rag_metrics)\n",
        "\n",
        "# Create comparison table\n",
        "comparison_df = pd.DataFrame(eval_results)\n",
        "comparison_df\n",
        "comparison_df.to_csv(\"evaluation_results.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "2HJ0IiBVWzI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def display_summary_comparison(example_id=0, output_file=sys.stdout):\n",
        "    article = test_dataset[example_id][\"article\"]\n",
        "    reference = test_dataset[example_id][\"highlights\"]\n",
        "\n",
        "    # Get summaries from different approaches\n",
        "    summarizer_base = pipeline(\"summarization\", model=\"google/flan-t5-base\")\n",
        "    summarizer_ft = pipeline(\"summarization\", model=\"/content/drive/MyDrive/flan-t5-summarization-final\")\n",
        "\n",
        "    base_summary = summarizer_base(\"Summarize: \" + article, max_length=128)[0][\"summary_text\"]\n",
        "    ft_summary = summarizer_ft(\"Summarize: \" + article, max_length=128)[0][\"summary_text\"]\n",
        "    rag_summary = rag_enhanced_summarization(article, \"/content/drive/MyDrive/flan-t5-summarization-final\")\n",
        "\n",
        "    output_file.write(\"=\"*80 + \"\\n\")\n",
        "    output_file.write(\"ARTICLE EXCERPT (first 300 chars):\\n\")\n",
        "    output_file.write(article[:300] + \"...\\n\")\n",
        "    output_file.write(\"\\nREFERENCE SUMMARY:\\n\")\n",
        "    output_file.write(reference + \"\\n\")\n",
        "    output_file.write(\"\\nBASE MODEL SUMMARY:\\n\")\n",
        "    output_file.write(base_summary + \"\\n\")\n",
        "    output_file.write(\"\\nFINE-TUNED MODEL SUMMARY:\\n\")\n",
        "    output_file.write(ft_summary + \"\\n\")\n",
        "    output_file.write(\"\\nRAG-ENHANCED SUMMARY:\\n\")\n",
        "    output_file.write(rag_summary + \"\\n\")\n",
        "    output_file.write(\"=\"*80 + \"\\n\")\n",
        "\n",
        "    # Calculate ROUGE scores for each approach\n",
        "    base_rouge = scorer.score(reference, base_summary)\n",
        "    ft_rouge = scorer.score(reference, ft_summary)\n",
        "    rag_rouge = scorer.score(reference, rag_summary)\n",
        "\n",
        "    scores_table = PrettyTable()\n",
        "    scores_table.field_names = [\"Model\", \"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\"]\n",
        "    scores_table.add_row([\"Base FLAN-T5\",\n",
        "                         f\"{base_rouge['rouge1'].fmeasure:.4f}\",\n",
        "                         f\"{base_rouge['rouge2'].fmeasure:.4f}\",\n",
        "                         f\"{base_rouge['rougeL'].fmeasure:.4f}\"])\n",
        "    scores_table.add_row([\"Fine-tuned FLAN-T5\",\n",
        "                         f\"{ft_rouge['rouge1'].fmeasure:.4f}\",\n",
        "                         f\"{ft_rouge['rouge2'].fmeasure:.4f}\",\n",
        "                         f\"{ft_rouge['rougeL'].fmeasure:.4f}\"])\n",
        "    scores_table.add_row([\"RAG-enhanced\",\n",
        "                         f\"{rag_rouge['rouge1'].fmeasure:.4f}\",\n",
        "                         f\"{rag_rouge['rouge2'].fmeasure:.4f}\",\n",
        "                         f\"{rag_rouge['rougeL'].fmeasure:.4f}\"])\n",
        "\n",
        "    output_file.write(str(scores_table) + \"\\n\")\n",
        "\n",
        "\n",
        "# Open the file in write mode and specify the location to save the output\n",
        "output_path = \"/content/drive/MyDrive/flan-t5-summarization-final/comparison_results.txt\"\n",
        "with open(output_path, \"w\") as output_file:\n",
        "    # Display comparisons for 3 different examples\n",
        "    for i in range(3):\n",
        "        display_summary_comparison(i, output_file)\n",
        "        output_file.write(\"\\n\")\n"
      ],
      "metadata": {
        "id": "7QXSMbdaXA-E"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}